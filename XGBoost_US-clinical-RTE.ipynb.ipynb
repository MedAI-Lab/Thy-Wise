{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary dependency libraries\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model performance test functions\n",
    "def performance_xgboost(xtrainup,ytrainup,xtrain,ytrain):\n",
    "    clf.fit(xtrainup, ytrainup)\n",
    "    # AUC\n",
    "    auc_scores = roc_auc_score(ytrain, clf.predict_proba(xtrain)[:,1])\n",
    "    print('auc = ', \"%.3f\"%auc_scores)\n",
    "    # accuracy\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy = accuracy_score(ytrain, clf.predict(xtrain))*100\n",
    "    print('accuracy = ', \"%.1f\"%accuracy)\n",
    "    print('------------------------')\n",
    "    # 混淆矩阵\n",
    "    import sklearn.metrics as sm\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    matrix = confusion_matrix(ytrain, clf.predict(xtrain), labels=None, sample_weight=None)\n",
    "    print('混淆矩阵为：')\n",
    "    print(matrix)\n",
    "    (tn,fp,fn,tp) = matrix.ravel()\n",
    "    print('tn=',tn)\n",
    "    print('fp=',fp)\n",
    "    print('fn=',fn)\n",
    "    print('tp=',tp)\n",
    "    print('------------------------')\n",
    "    sensitivity = (tp/(tp+fn))*100\n",
    "    specificity = (tn/(fp+tn))*100\n",
    "    PPV=tp/(tp+fp)*100\n",
    "    NPV=tn/(fn+tn)*100\n",
    "    print(f'PPV = {\"%.1f\"%PPV}\\n({tp}/{(tp+fp)})')\n",
    "    print(f'NPV = {\"%.1f\"%NPV}\\n({tn}/{(fn+tn)})')\n",
    "    print(f'sensitivity = {\"%.1f\"%sensitivity}\\n({tp}/{(tp+fn)})')\n",
    "    print(f'specificity = {\"%.1f\"%specificity}\\n({tn}/{(fp+tn)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinzh\\anaconda3\\lib\\site-packages\\imblearn\\utils\\_validation.py:299: UserWarning: After over-sampling, the number of samples (2000) in class 1 will be larger than the number of samples in the majority class (class #0 -> 1582)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Reading data\n",
    "train = pd.read_csv('F:/Onedrive/JIMMY/Python/Notebook/data_set/inuse/train.csv',encoding='gbk')\n",
    "test = pd.read_csv('F:/Onedrive/JIMMY/Python/Notebook/data_set/inuse/test.csv',encoding='gbk')\n",
    "\n",
    "xtrain = train.iloc[:,1:15]\n",
    "xtest = test.iloc[:,1:15]\n",
    "ytrain = train.iloc[:,-1]\n",
    "ytest = test.iloc[:,-1]\n",
    "\n",
    "# Up-sampling processing\n",
    "sm = SMOTE(sampling_strategy={1: 2000},random_state=100) \n",
    "xtrainup,ytrainup = sm.fit_resample(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8, gamma=0.06, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.01, max_delta_step=0, max_depth=2,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=433, n_jobs=-1, nthread=4, num_parallel_tree=1,\n",
       "              random_state=27, reg_alpha=0.07, reg_lambda=1,\n",
       "              scale_pos_weight=1.4, seed=27, subsample=1, tree_method='auto',\n",
       "              use_label_encoder=False, validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Instantiation\n",
    "clf = XGBClassifier(use_label_encoder=False\n",
    "                   ,learning_rate =0.01  \n",
    "                   ,n_estimators=433\n",
    "                   ,max_depth=2\n",
    "                   ,min_child_weight=1\n",
    "                   ,gamma=0.06\n",
    "                   ,subsample=1\n",
    "                   ,colsample_bytree=0.8\n",
    "                   ,objective= 'binary:logistic'\n",
    "                   ,scale_pos_weight=1.4\n",
    "                   ,nthread=4\n",
    "                   ,reg_alpha=0.07\n",
    "                   ,tree_method='auto'\n",
    "                   ,seed=27\n",
    "                   ,n_jobs=-1) \n",
    "clf.fit(xtrainup, ytrainup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "auc =  0.945\n",
      "accuracy =  86.0\n",
      "------------------------\n",
      "混淆矩阵为：\n",
      "[[1318  264]\n",
      " [  39  547]]\n",
      "tn= 1318\n",
      "fp= 264\n",
      "fn= 39\n",
      "tp= 547\n",
      "------------------------\n",
      "PPV = 67.4\n",
      "(547/811)\n",
      "NPV = 97.1\n",
      "(1318/1357)\n",
      "sensitivity = 93.3\n",
      "(547/586)\n",
      "specificity = 83.3\n",
      "(1318/1582)\n"
     ]
    }
   ],
   "source": [
    "# Testing the model on the training set\n",
    "performance_xgboost(xtrainup,ytrainup,xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "auc =  0.930\n",
      "accuracy =  84.3\n",
      "------------------------\n",
      "混淆矩阵为：\n",
      "[[546 120]\n",
      " [ 26 238]]\n",
      "tn= 546\n",
      "fp= 120\n",
      "fn= 26\n",
      "tp= 238\n",
      "------------------------\n",
      "PPV = 66.5\n",
      "(238/358)\n",
      "NPV = 95.5\n",
      "(546/572)\n",
      "sensitivity = 90.2\n",
      "(238/264)\n",
      "specificity = 82.0\n",
      "(546/666)\n"
     ]
    }
   ],
   "source": [
    "# Testing the model on the internal validation set\n",
    "performance_xgboost(xtrainup,ytrainup,xtest,ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
