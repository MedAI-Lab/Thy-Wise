{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary dependency libraries\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model performance test functions\n",
    "def performance_xgboost(xtrainup,ytrainup,xtrain,ytrain):\n",
    "    clf.fit(xtrainup, ytrainup)\n",
    "    # AUC\n",
    "    auc_scores = roc_auc_score(ytrain, clf.predict_proba(xtrain)[:,1])\n",
    "    print('auc = ', \"%.3f\"%auc_scores)\n",
    "    # accuracy\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy = accuracy_score(ytrain, clf.predict(xtrain))*100\n",
    "    print('accuracy = ', \"%.1f\"%accuracy)\n",
    "    print('------------------------')\n",
    "    # 混淆矩阵\n",
    "    import sklearn.metrics as sm\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    matrix = confusion_matrix(ytrain, clf.predict(xtrain), labels=None, sample_weight=None)\n",
    "    print('混淆矩阵为：')\n",
    "    print(matrix)\n",
    "    (tn,fp,fn,tp) = matrix.ravel()\n",
    "    print('tn=',tn)\n",
    "    print('fp=',fp)\n",
    "    print('fn=',fn)\n",
    "    print('tp=',tp)\n",
    "    print('------------------------')\n",
    "    sensitivity = (tp/(tp+fn))*100\n",
    "    specificity = (tn/(fp+tn))*100\n",
    "    PPV=tp/(tp+fp)*100\n",
    "    NPV=tn/(fn+tn)*100\n",
    "    print(f'PPV = {\"%.1f\"%PPV}\\n({tp}/{(tp+fp)})')\n",
    "    print(f'NPV = {\"%.1f\"%NPV}\\n({tn}/{(fn+tn)})')\n",
    "    print(f'sensitivity = {\"%.1f\"%sensitivity}\\n({tp}/{(tp+fn)})')\n",
    "    print(f'specificity = {\"%.1f\"%specificity}\\n({tn}/{(fp+tn)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinzh\\anaconda3\\lib\\site-packages\\imblearn\\utils\\_validation.py:299: UserWarning: After over-sampling, the number of samples (2000) in class 1 will be larger than the number of samples in the majority class (class #0 -> 1582)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Reading data\n",
    "train = pd.read_csv('F:/Onedrive/JIMMY/Python/Notebook/data_set/inuse/train.csv',encoding='gbk')\n",
    "test = pd.read_csv('F:/Onedrive/JIMMY/Python/Notebook/data_set/inuse/test.csv',encoding='gbk')\n",
    "validation = pd.read_csv('F:/Onedrive/JIMMY/Python/Notebook/data_set/inuse/validation_new04.csv',encoding='gbk')\n",
    "\n",
    "xtrain = train.iloc[:,9:14]\n",
    "xtest = test.iloc[:,9:14]\n",
    "xv = validation.iloc[:,9:14]\n",
    "ytrain = train.iloc[:,-1]\n",
    "ytest = test.iloc[:,-1]\n",
    "yv = validation.iloc[:,-1]\n",
    "\n",
    "# Up-sampling processing\n",
    "sm = SMOTE(sampling_strategy={1: 2000},random_state=100) \n",
    "xtrainup,ytrainup = sm.fit_resample(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.01, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=0, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=400, n_jobs=-1, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=1, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='auto', use_label_encoder=False,\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Instantiation\n",
    "clf = XGBClassifier(n_estimators=400,\n",
    "                     max_depth=5,\n",
    "                     min_child_weight=0,\n",
    "                     gamma=0,\n",
    "                     subsample=1,\n",
    "                     colsample_bytree=0.7,\n",
    "                     reg_alpha=1,\n",
    "                     learning_rate =0.01,\n",
    "                     use_label_encoder=False,\n",
    "                     objective= 'binary:logistic',\n",
    "                     tree_method='auto',\n",
    "                     n_jobs=-1\n",
    "                    ,scale_pos_weight=1) \n",
    "clf.fit(xtrainup, ytrainup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "auc =  0.916\n",
      "accuracy =  83.5\n",
      "------------------------\n",
      "混淆矩阵为：\n",
      "[[1270  312]\n",
      " [  45  541]]\n",
      "tn= 1270\n",
      "fp= 312\n",
      "fn= 45\n",
      "tp= 541\n",
      "------------------------\n",
      "PPV = 63.4\n",
      "(541/853)\n",
      "NPV = 96.6\n",
      "(1270/1315)\n",
      "sensitivity = 92.3\n",
      "(541/586)\n",
      "specificity = 80.3\n",
      "(1270/1582)\n"
     ]
    }
   ],
   "source": [
    "# Testing the model on the training set\n",
    "performance_xgboost(xtrainup,ytrainup,xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "auc =  0.899\n",
      "accuracy =  82.7\n",
      "------------------------\n",
      "混淆矩阵为：\n",
      "[[531 135]\n",
      " [ 26 238]]\n",
      "tn= 531\n",
      "fp= 135\n",
      "fn= 26\n",
      "tp= 238\n",
      "------------------------\n",
      "PPV = 63.8\n",
      "(238/373)\n",
      "NPV = 95.3\n",
      "(531/557)\n",
      "sensitivity = 90.2\n",
      "(238/264)\n",
      "specificity = 79.7\n",
      "(531/666)\n"
     ]
    }
   ],
   "source": [
    "# Testing the model on the internal validation set\n",
    "performance_xgboost(xtrainup,ytrainup,xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "auc =  0.883\n",
      "accuracy =  72.8\n",
      "------------------------\n",
      "混淆矩阵为：\n",
      "[[415 217]\n",
      " [ 19 216]]\n",
      "tn= 415\n",
      "fp= 217\n",
      "fn= 19\n",
      "tp= 216\n",
      "------------------------\n",
      "PPV = 49.9\n",
      "(216/433)\n",
      "NPV = 95.6\n",
      "(415/434)\n",
      "sensitivity = 91.9\n",
      "(216/235)\n",
      "specificity = 65.7\n",
      "(415/632)\n"
     ]
    }
   ],
   "source": [
    "# Testing the model on the external validation set\n",
    "performance_xgboost(xtrainup,ytrainup,xv,yv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
